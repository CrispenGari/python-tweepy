{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "Topic: `A Community-based Real-Time Service Delivery Sentiment Analysis Data Fetching.`\n",
    "\n",
    "Date: `2022/06/16`\n",
    "\n",
    "Programming Language: `python`\n",
    "\n",
    "Main: `Natural Language Processing (NLP)`\n",
    "\n",
    "___\n",
    "\n",
    "\n",
    "\n",
    "### Service delivery SA data Scrapping\n",
    "\n",
    "In this notebook we are ging to use the `twitter` API to collect data for sentiment classification task. We are going to use python programming language to scrap the data from twitter. We are going to use `tweepy` together with api keys. \n",
    "\n",
    "\n",
    "### Installation of `tweepy`\n",
    "In the following code cell we are going to install `tweepy`. This package allows us to interact twitter using python programming language using.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweepy\n",
      "  Downloading tweepy-4.10.0-py3-none-any.whl (94 kB)\n",
      "Collecting requests<3,>=2.27.0\n",
      "  Using cached requests-2.28.0-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in c:\\users\\crisp\\anaconda3\\lib\\site-packages (from tweepy) (1.3.0)\n",
      "Collecting oauthlib<4,>=3.2.0\n",
      "  Using cached oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\crisp\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\crisp\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2021.5.30)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Using cached charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\crisp\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2.10)\n",
      "Installing collected packages: charset-normalizer, requests, oauthlib, tweepy\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.24.0\n",
      "    Uninstalling requests-2.24.0:\n",
      "      Successfully uninstalled requests-2.24.0\n",
      "  Attempting uninstall: oauthlib\n",
      "    Found existing installation: oauthlib 3.1.1\n",
      "    Uninstalling oauthlib-3.1.1:\n",
      "      Successfully uninstalled oauthlib-3.1.1\n",
      "Successfully installed charset-normalizer-2.0.12 oauthlib-3.2.0 requests-2.28.0 tweepy-4.10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "conda 4.10.3 requires ruamel_yaml_conda>=0.11.14, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing TextBlob\n",
    "\n",
    "We are going to use textblob in this notebook to name tweets sentiments based on a condition. During data scrapping on tweeter our tweets will not be labeled `positive`, `negative` or `nuetral` we have to do this on our own by the help of the [TextBlob Library](https://textblob.readthedocs.io/en/dev/), which is a library for processing text in python. We are going to use this library to group our text based on `polarity` value either the text is `positive`, `negative` or `nuetral`.\n",
    "\n",
    "\n",
    "> TextBlob returns `polarity` and `subjectivity` of a sentence. Polarity lies between `[-1,1]`, `-1` defines a negative sentiment and `1` defines a positive sentiment. Negation words reverse the polarity. TextBlob has semantic labels that help with fine-grained analysis. For example — emoticons, exclamation mark, emojis, etc. Subjectivity lies between `[0,1]`. Subjectivity quantifies the amount of personal opinion and factual information contained in the text. The higher subjectivity means that the text contains personal opinion rather than factual information. TextBlob has one more parameter — intensity. TextBlob calculates subjectivity by looking at the ‘intensity’. Intensity determines if a word modifies the next word. For English, adverbs are used as modifiers (‘very good’).\n",
    "\n",
    "We are only going to make use of the text `polarity` and create labels for our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "Requirement already satisfied: nltk>=3.1; python_version >= \"3\" in c:\\users\\crisp\\anaconda3\\lib\\site-packages (from textblob) (3.5)\n",
      "Requirement already satisfied: joblib in c:\\users\\crisp\\anaconda3\\lib\\site-packages (from nltk>=3.1; python_version >= \"3\"->textblob) (0.17.0)\n",
      "Requirement already satisfied: click in c:\\users\\crisp\\anaconda3\\lib\\site-packages (from nltk>=3.1; python_version >= \"3\"->textblob) (8.0.3)\n",
      "Requirement already satisfied: regex in c:\\users\\crisp\\anaconda3\\lib\\site-packages (from nltk>=3.1; python_version >= \"3\"->textblob) (2020.10.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\crisp\\anaconda3\\lib\\site-packages (from nltk>=3.1; python_version >= \"3\"->textblob) (4.50.2)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\crisp\\anaconda3\\lib\\site-packages (from click->nltk>=3.1; python_version >= \"3\"->textblob) (0.4.4)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.17.1\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages\n",
    "In the following code cell we are going to import packages that we are going to use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\crisp/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to C:\\Users\\crisp/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tweepy as tp\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import textblob\n",
    "import nltk\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"words\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "conda install -c conda-forge textblob\n",
    "# then\n",
    "python -m textblob.download_corpora\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Keys\n",
    "\n",
    "Inorder to scrap data from twitter using `tweepy` we need to have a [twitter-developer-account](https://developer.twitter.com/en) which can be created [here](https://developer.twitter.com/en). Inoder to get these keys you need to:\n",
    "\n",
    "1. login to your twitter developer account\n",
    "2. create an application\n",
    "3. get the following keys\n",
    "    \t* `API_KEY`\n",
    "        * `API_SECRET`\n",
    "        * `ACCESS_TOKEN`\n",
    "        *`ACCESS_TOKEN_SECRET`\n",
    "        \n",
    "After getting these keys we are gong to create a file called `keys.json` which is where we are gong to store our keys instead of displaying them here in this notebook. The `keys.json` file will look as follows:\n",
    "\n",
    "```json\n",
    "{\n",
    " \"API_KEY\" : \"<YOUR_API_KEY>\",\n",
    " \"API_SECRET\": \"<YOUR_API_SECRETE>\",\n",
    " \"ACCESS_TOKEN\": \"<YOUR_ACCESS_TOKEN>\",\n",
    " \"ACCESS_TOKEN_SECRET\": \"<YOUR_ACCESS_TOKEN_SECRETE>\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading `KEYS`.\n",
    "\n",
    "In the following code cell we are going to load `KEYS` from an external file `keys.json`. The reason I'm loading these keys from an external file it's because i dont want them to be visible in this notebook and I'm going to add the file name in the `.gitignore` file so that when this code is pushed to `github` the `keys.json` won't be uploaded and noone will have access to our `keys` to use them on our behalf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('keys.json', 'r') as reader:\n",
    "    keys = json.loads(reader.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keys Type\n",
    "In the following code cell we are going to create a data class type called `Keys`. This datatype class will store our keys as an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keys:\n",
    "    API_KEY             = keys['API_KEY']\n",
    "    API_SECRET          = keys['API_SECRET']\n",
    "    ACCESS_TOKEN        = keys['ACCESS_TOKEN']\n",
    "    ACCESS_TOKEN_SECRET = keys['ACCESS_TOKEN_SECRET']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating `api` object\n",
    "\n",
    "We are going to autheticate to our twitter developer account using the `OAuthHandler` class. This class takes in two arguments which are:\n",
    "\n",
    "* `API_KEY`\n",
    "* `API_SECRET`\n",
    "\n",
    "On our object `auth` we are going to use the method called `set_access_token` that takes in the followng arguments.\n",
    "* `ACCESS_TOKEN`\n",
    "* `ACCESS_TOKEN_SECRET`\n",
    "\n",
    "We are then going to create an `api` object and pass in the `authentication`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tp.OAuthHandler(Keys.API_KEY, Keys.API_SECRET)\n",
    "auth.set_access_token(Keys.ACCESS_TOKEN, Keys.ACCESS_TOKEN_SECRET)\n",
    "api = tp.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying data on Twitter\n",
    "\n",
    "We are going to scrap the data based on twitter wich is related to service delivery. So our query(q) value will be based on the hashtag `#servicedelivery`. We are going to menton the count and use the tweepy `Cursor` class to retrieve the `count` tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"#servicedelivery\"\n",
    "COUNT = 1_000\n",
    "\n",
    "sd_tweets = tp.Cursor(api.search_tweets, q=q, lang=\"en\", tweet_mode=\"extended\", include_entities=True).items(COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Class Tweet\n",
    "In the following code cell we are going to create a datatype called `Tweet`. This datatype will contain properties that we are interested in on a single `tweet` object. On each and every tweet we are going to be intrested with the following attributes:\n",
    "\n",
    "1. `id` - the tweet id\n",
    "\n",
    "2. `created_at` - the date a tweet was created\n",
    "\n",
    "3. `username` - who tweeted this tweet\n",
    "\n",
    "4. `text` - the text content of the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweet:\n",
    "    def __init__(self, id:str, created_at:str, username:str, text:str):\n",
    "        self.id = id\n",
    "        self.created_at = created_at\n",
    "        self.username = username\n",
    "        self.text = text\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Tweet <{self.id}>\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Tweet <{self.id}>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet._json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapping the Tweets\n",
    "\n",
    "In the following code cell we are going to scrap the tweets and store the in a `tweets` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = list()\n",
    "\n",
    "for tweet in sd_tweets:\n",
    "    try:\n",
    "        tweet = tweet._json\n",
    "        data = (tweet['id'], tweet['created_at'], tweet['user']['screen_name'], tweet['full_text'])\n",
    "        tweets.append(Tweet(*data))\n",
    "    except tp.TweepError as e:\n",
    "        print(e.reason)\n",
    "        continue\n",
    "    except StopIteration as s:\n",
    "        print(str(s))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking a single `tweet` example\n",
    "In the following code cell we are going to check a single tweet example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#RaiseYourVoice #June16 #YouthDay\\n\"...the nation owes you a clear policy and practical measures to ensure that the younger generation contributes to, and benefit from, our new democracy.\" - @NelsonMandela\\n#YouthMonth #Asivikelane #RoofIsMyRight #servicedelivery https://t.co/D3ljTihtNv'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "294"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Features (text) Cleaning\n",
    "\n",
    "Our tweets was obtained using `scrapping` if we look at our example texts we can see that we have hastags `#`, mentions `@user`, numbers `123`, url's `http://google.com/whatever` etc. These things does not add any meaning to our text. We are going to create a preprocessing function that will be able to remove all hashtags, mentions, urls, numbers as well as expanding the words like `I'm` to `I am` so that we make our text so clean. We are also going to remove single letter that means nothing and convert everything word to lowercase for example let's have a look at the following sentence :\n",
    "\n",
    "```\n",
    "I'm working with a model downloaded on http://google.com/whatever #100daysofcode created using AI by @username5 e h in 2015.\n",
    "```\n",
    "\n",
    "When we clean this text we want it to look as follows:\n",
    "\n",
    "```\n",
    "i am working with a model downloaded on created using ai by in.\n",
    "```\n",
    "\n",
    "The above sentence does not make any sense to human being to to deep learning model it does make sense. Because deep learning models learns the context in the sentence not the meaning of the sentence.\n",
    "\n",
    "### Is text cleaning going to improve model metrics?\n",
    "\n",
    "Cleaning features also known as a step to \"feature extration\" is a very important step in machine learning models. It helps us to reduce noise in our features so that our model instead of forcusing on learning numbers, hashtags and mentions it will just focus on the text which is what maters. This also reduces the size of the `vocabulary`. In NLP we have an important consept called `vocabulary` which i'm going to explain it more about it later in the model training notebook. But we must know that text cleaning reduces the size of the vocabulary and improve the model training speed.\n",
    "\n",
    "In the code cells that follows we are going to make use of the `nltk` and `re` packages to clean our text. We are going to create a function that will does the text cleaning for us and this function will be called `clean_sentence`. This function instead of removing just the noise from the sentence it will also expand contracted words such as `ain't` to `are not` for example.\n",
    "\n",
    "The text-cleaning functions was found on [CrispenGari/ml-utils](https://github.com/CrispenGari/ml-utils/tree/main/text-cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase:str)->str:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        phrase (str): takes in a word like I'm\n",
    "\n",
    "    Returns:\n",
    "        string: a decontracted word like I am\n",
    "    \"\"\"\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sent:str)->str:\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sent (str): an uncleaned sentence with text, punctuations, numbers and non-english words\n",
    "    Returns:\n",
    "        str: returns a cleaned sentence with only english words in it.\n",
    "    \"\"\"\n",
    "    sent = sent.lower() # converting the text to lower case\n",
    "    sent = re.sub(r'(@|#)([A-Za-z0-9]+)', ' ', sent) # removing tags and mentions (there's no right way of doing it with regular expression but this will try)\n",
    "    sent = re.sub(r\"([A-Za-z0-9]+[.-_])*[A-Za-z0-9]+@[A-Za-z0-9-]+(\\.[A-Z|a-z]{2,})+\", \" \", sent) # removing emails\n",
    "    sent = re.sub(r'https?\\S+', ' ', sent, flags=re.MULTILINE) # removing url's\n",
    "    sent = re.sub(r'\\d', ' ', sent) # removing none word characters\n",
    "    sent = re.sub(r'[^\\w\\s\\']', ' ', sent) # removing punctuations except for \"'\" in words like I'm\n",
    "    sent = re.sub(r'\\s+', ' ', sent).strip() # remove more than one space\n",
    "    words = list()\n",
    "    eng = set(nltk.corpus.words.words())\n",
    "    for word in sent.split(' '):\n",
    "        words.append(decontracted(word)) # replace word's like \"i'm -> i am\"\n",
    "    return \" \".join(w for w in words if w.lower() in eng or not w.isalpha()) # removing non-english words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TextBlob’s output for a polarity task is a float within the range `[-1.0, 1.0]` where `-1.0` is a negative polarity and `1.0` is positive. This score can also be equal to `0`, which stands for a neutral evaluation of a statement as it doesn’t contain any words from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "later on we will customize for neutral based on the absolute threshold to balance the data.\n",
    "\"\"\"\n",
    "\n",
    "def create_label(text:str)->str:\n",
    "    blob = TextBlob(text)\n",
    "    polarity =  blob.sentiment.polarity\n",
    "    if polarity == 0.0:\n",
    "        return \"neutral\"\n",
    "    elif polarity < 0.0:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('positive', 'neutral', 'negative')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_label(\"This is lovely\"), create_label(\"I'm not saying anything\"), create_label(\"this is boring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Label\n",
    "\n",
    "Next we are going to create a categorical label based on the condition that if a given sentiment label is `negative` our categorical label will be `1` if our sentiment class label is `neutral` our categorical label will be `0` and `1` when it is positive. We are going to make create a python `lambda` function called `categorical_label`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_label = lambda x: 1 if x == \"negative\" else 0 if x == \"neutral\" else 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Tweets\n",
    "\n",
    "We are then going to clean our tweets text, give them categorical and class labels then save the the data in a `csv` file. Our `csv` file will have the following columns.\n",
    "\n",
    "1. `created_at` - the date when the tweet was created\n",
    "\n",
    "2. `text` - the tweet cleaned text\n",
    "\n",
    "3. `label` - the class label which can be `positive`, `negative` or `neutral`\n",
    "\n",
    "4. `categorical_label` - an integer value either `2`, `1` or `0` for `positive`, `negative` or `neutral` class labels respectively\n",
    "\n",
    "5. `username` - the user who wrote the tweet.\n",
    "\n",
    "6. `id` - the tweet id.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets = list()\n",
    "\n",
    "for twt in tweets:\n",
    "    twt_txt = clean_sentence(twt.text)\n",
    "    twt_id = twt.id\n",
    "    twt_create_at = twt.created_at\n",
    "    twt_username = twt.username\n",
    "    \n",
    "    # labels\n",
    "    twt_label = create_label(twt_txt)\n",
    "    twt_categorical_label = categorical_label(twt_label)\n",
    "    \n",
    "    cleaned_tweets.append(tuple([twt_id, twt_create_at, twt_username, twt_txt, twt_label, twt_categorical_label]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns\n",
    "In the following code cell we are going to define the columns name that we are going to need in our `.csv` file as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = np.array(['id', 'created_at', 'username', 'text', 'label', 'categorical_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe\n",
    "In the following column we are going to create a dataframe base on our `cleaned_tweets` list and check the first `10` rows of our data as follows:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>categorical_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1537382368381632513</td>\n",
       "      <td>Thu Jun 16 10:31:34 +0000 2022</td>\n",
       "      <td>Planact_NGO</td>\n",
       "      <td>the nation you a clear policy and practical to...</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1537338184312528896</td>\n",
       "      <td>Thu Jun 16 07:36:00 +0000 2022</td>\n",
       "      <td>vanandasoapery</td>\n",
       "      <td>appreciation tweet to mayor of municipality an...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1537321364511772673</td>\n",
       "      <td>Thu Jun 16 06:29:10 +0000 2022</td>\n",
       "      <td>realmphomalgas</td>\n",
       "      <td>appreciation tweet to mayor of municipality an...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1537152631684186114</td>\n",
       "      <td>Wed Jun 15 19:18:40 +0000 2022</td>\n",
       "      <td>ThePatriotOfZAR</td>\n",
       "      <td>appreciation tweet to mayor of municipality an...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1537150068284870656</td>\n",
       "      <td>Wed Jun 15 19:08:29 +0000 2022</td>\n",
       "      <td>TonyBeamish</td>\n",
       "      <td>'it s now time to change the culture within st...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1537106399167139843</td>\n",
       "      <td>Wed Jun 15 16:14:58 +0000 2022</td>\n",
       "      <td>Moneyweb</td>\n",
       "      <td>'it s now time to change the culture within st...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1537074114199953414</td>\n",
       "      <td>Wed Jun 15 14:06:40 +0000 2022</td>\n",
       "      <td>dejiafolabiesq</td>\n",
       "      <td>if not effectively or the superior official is...</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1537050019391492096</td>\n",
       "      <td>Wed Jun 15 12:30:56 +0000 2022</td>\n",
       "      <td>sagren</td>\n",
       "      <td>_mayor why the it is just as it should be as i...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1537029414386651138</td>\n",
       "      <td>Wed Jun 15 11:09:03 +0000 2022</td>\n",
       "      <td>MavoSthe</td>\n",
       "      <td>thank you so much you are such an my brother m...</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1536985440674549761</td>\n",
       "      <td>Wed Jun 15 08:14:19 +0000 2022</td>\n",
       "      <td>Bizmalawi</td>\n",
       "      <td>know about our free local delivery paint colou...</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                      created_at         username  \\\n",
       "0  1537382368381632513  Thu Jun 16 10:31:34 +0000 2022      Planact_NGO   \n",
       "1  1537338184312528896  Thu Jun 16 07:36:00 +0000 2022   vanandasoapery   \n",
       "2  1537321364511772673  Thu Jun 16 06:29:10 +0000 2022   realmphomalgas   \n",
       "3  1537152631684186114  Wed Jun 15 19:18:40 +0000 2022  ThePatriotOfZAR   \n",
       "4  1537150068284870656  Wed Jun 15 19:08:29 +0000 2022      TonyBeamish   \n",
       "5  1537106399167139843  Wed Jun 15 16:14:58 +0000 2022         Moneyweb   \n",
       "6  1537074114199953414  Wed Jun 15 14:06:40 +0000 2022   dejiafolabiesq   \n",
       "7  1537050019391492096  Wed Jun 15 12:30:56 +0000 2022           sagren   \n",
       "8  1537029414386651138  Wed Jun 15 11:09:03 +0000 2022         MavoSthe   \n",
       "9  1536985440674549761  Wed Jun 15 08:14:19 +0000 2022        Bizmalawi   \n",
       "\n",
       "                                                text     label  \\\n",
       "0  the nation you a clear policy and practical to...  positive   \n",
       "1  appreciation tweet to mayor of municipality an...   neutral   \n",
       "2  appreciation tweet to mayor of municipality an...   neutral   \n",
       "3  appreciation tweet to mayor of municipality an...   neutral   \n",
       "4  'it s now time to change the culture within st...   neutral   \n",
       "5  'it s now time to change the culture within st...   neutral   \n",
       "6  if not effectively or the superior official is...  negative   \n",
       "7  _mayor why the it is just as it should be as i...   neutral   \n",
       "8  thank you so much you are such an my brother m...  positive   \n",
       "9  know about our free local delivery paint colou...  positive   \n",
       "\n",
       "   categorical_label  \n",
       "0                  2  \n",
       "1                  0  \n",
       "2                  0  \n",
       "3                  0  \n",
       "4                  0  \n",
       "5                  0  \n",
       "6                  1  \n",
       "7                  0  \n",
       "8                  2  \n",
       "9                  2  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.DataFrame(cleaned_tweets, columns=columns, index=None)\n",
    "dataframe.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving our data \n",
    "\n",
    "We are going to save our cleaned data in a `csv` file with the name `clean_tweets.csv` we are fong to use the pandas method on the dataframe called `to_csv` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "save_path = 'clean_tweets.csv'\n",
    "\n",
    "dataframe.to_csv(save_path)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next\n",
    "\n",
    "In the next notebook we are going to create an `Artificial Neural Network (ANN)` model that will classifies our text into sentiment wether, positive, positive or nuetral. Given a text from the user our model will classify sentiments to positive, positive or nuetral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. [tweepy docs](https://docs.tweepy.org/en/stable/authentication.html)\n",
    "2. [textblob-docs](https://textblob.readthedocs.io/en/dev/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
