{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kM9Zc0gV0vKC"
      },
      "source": [
        "___\n",
        "\n",
        "Topic: `A Community-based Real-Time Service Delivery Sentiment Analysis Data Fetching.`\n",
        "\n",
        "Date: `2022/06/16`\n",
        "\n",
        "Programming Language: `python`\n",
        "\n",
        "Main: `Natural Language Processing (NLP)`\n",
        "\n",
        "___\n",
        "\n",
        "\n",
        "\n",
        "### Service delivery SA data Scrapping\n",
        "\n",
        "In this notebook we are ging to use the `twitter` API to collect data for sentiment classification task. We are going to use python programming language to scrap the data from twitter. We are going to use `tweepy` together with api keys. \n",
        "\n",
        "\n",
        "### Installation of `tweepy`\n",
        "In the following code cell we are going to install the latest version of `tweepy`. This package allows us to interact twitter using python programming language using.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYS2rKL00vKM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e58e059f-569f-4ad2-db62-4c10fbc4bea0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.7/dist-packages (3.10.0)\n",
            "Collecting tweepy\n",
            "  Downloading tweepy-4.10.0-py3-none-any.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (3.2.0)\n",
            "Collecting requests<3,>=2.27.0\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy) (1.24.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy) (2022.6.15)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy) (2.1.0)\n",
            "Installing collected packages: requests, tweepy\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: tweepy\n",
            "    Found existing installation: tweepy 3.10.0\n",
            "    Uninstalling tweepy-3.10.0:\n",
            "      Successfully uninstalled tweepy-3.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.28.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed requests-2.28.1 tweepy-4.10.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tweepy --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why tweepy?\n",
        "\n",
        "We can use libraries like `selenium` to scrape the data for our task but tweepy comes with the following advantages:\n",
        "\n",
        "* Provides many features about a given tweet (e.g. information about a tweet’s geographical location, etc.)\n",
        "* Easy to use.\n",
        "* This is an official way of getting tweets from tweeter especially for research purposes.\n",
        "* It has a well written docummentation.\n",
        "\n",
        "However, tweepy comes with some limitations such as:\n",
        "\n",
        "* Limiting API requests.\n"
      ],
      "metadata": {
        "id": "McNtBvBPjEXu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "958xIfUY0vKU"
      },
      "source": [
        "### Installing TextBlob\n",
        "\n",
        "We are going to use textblob in this notebook to name tweets sentiments based on a condition. During data scrapping on tweeter our tweets will not be labeled `positive`, `negative` or `nuetral` we have to do this on our own by the help of the [TextBlob Library](https://textblob.readthedocs.io/en/dev/), which is a library for processing text in python. We are going to use this library to group our text based on `polarity` value either the text is `positive`, `negative` or `nuetral`.\n",
        "\n",
        "\n",
        "> TextBlob returns `polarity` and `subjectivity` of a sentence. Polarity lies between `[-1,1]`, `-1` defines a negative sentiment and `1` defines a positive sentiment. Negation words reverse the polarity. TextBlob has semantic labels that help with fine-grained analysis. For example — emoticons, exclamation mark, emojis, etc. Subjectivity lies between `[0,1]`. Subjectivity quantifies the amount of personal opinion and factual information contained in the text. The higher subjectivity means that the text contains personal opinion rather than factual information. TextBlob has one more parameter — intensity. TextBlob calculates subjectivity by looking at the ‘intensity’. Intensity determines if a word modifies the next word. For English, adverbs are used as modifiers (‘very good’).\n",
        "\n",
        "We are only going to make use of the text `polarity` and create labels for our dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rX9glV70vKV",
        "outputId": "dd155a5d-e9c1-43d0-d81c-3a987b4a028f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (4.64.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install textblob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BqgT6Yn0vKW"
      },
      "source": [
        "### Importing packages\n",
        "In the following code cell we are going to import packages that we are going to use in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "4An5qFaM0vKX",
        "outputId": "4b8677d5-2c9c-4d18-9433-85e897d254f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'4.10.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import tweepy as tp\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import uuid\n",
        "import textblob\n",
        "import nltk\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"words\")\n",
        "\n",
        "tp.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nycjxl6B0vKZ"
      },
      "source": [
        "### File System\n",
        "\n",
        "We are going to store and load data from google drive so, we need to mount the goggle drive. In the following code cell we are mounting the google drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive, files\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKkslu7e2G0w",
        "outputId": "933712b4-ac9e-4970-9696-82a7da07e905"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paths\n",
        "\n",
        "In the following code cell we are going to define the paths where all our files will be stored in the google drive."
      ],
      "metadata": {
        "id": "iQ0S_C2b2W5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = '/content/drive/My Drive/Service Delivery'\n",
        "\n",
        "assert os.path.exists(base_dir), f\"The path '{base_dir}' does not exists, check if you have mounted the google drive.\""
      ],
      "metadata": {
        "id": "2AbwEHzX2ghH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67Pc_BnD0vKb"
      },
      "source": [
        "### Getting Keys\n",
        "\n",
        "Inorder to scrap data from twitter using `tweepy` we need to have a [twitter-developer-account](https://developer.twitter.com/en) which can be created [here](https://developer.twitter.com/en). Inoder to get these keys you need to:\n",
        "\n",
        "1. login to your twitter developer account\n",
        "2. create an application\n",
        "3. get the following keys\n",
        "    * `API_KEY`\n",
        "    * `API_SECRET`\n",
        "    * `ACCESS_TOKEN`\n",
        "    * `ACCESS_TOKEN_SECRET`\n",
        "        \n",
        "After getting these keys we are gong to create a file called `keys.json` which is where we are gong to store our keys instead of displaying them here in this notebook. The `keys.json` file will look as follows:\n",
        "\n",
        "```json\n",
        "{\n",
        " \"API_KEY\" : \"<YOUR_API_KEY>\",\n",
        " \"API_SECRET\": \"<YOUR_API_SECRETE>\",\n",
        " \"ACCESS_TOKEN\": \"<YOUR_ACCESS_TOKEN>\",\n",
        " \"ACCESS_TOKEN_SECRET\": \"<YOUR_ACCESS_TOKEN_SECRETE>\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKUn-5vx0vKc"
      },
      "source": [
        "### Loading `KEYS`.\n",
        "\n",
        "In the following code cell we are going to load `KEYS` from an external file `keys.json`. The reason I'm loading these keys from an external file it's because i dont want them to be visible in this notebook and I'm going to add the file name in the `.gitignore` file so that when this code is pushed to `github` the `keys.json` won't be uploaded and noone will have access to our `keys` to use them on our behalf."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nByMldPQ0vKe"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(base_dir, 'keys.json'), 'r') as reader:\n",
        "    keys = json.loads(reader.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Gje0dHZ0vKf"
      },
      "source": [
        "### Keys Type\n",
        "In the following code cell we are going to create a data class type called `Keys`. This datatype class will store our keys as an object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cHmOB9D0vKf"
      },
      "outputs": [],
      "source": [
        "class Keys:\n",
        "    API_KEY             = keys['API_KEY']\n",
        "    API_SECRET          = keys['API_SECRET']\n",
        "    ACCESS_TOKEN        = keys['ACCESS_TOKEN']\n",
        "    ACCESS_TOKEN_SECRET = keys['ACCESS_TOKEN_SECRET']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itMdntw10vKg"
      },
      "source": [
        "### Creating `api` object\n",
        "\n",
        "We are going to autheticate to our twitter developer account using the `OAuthHandler` class. This class takes in two arguments which are:\n",
        "\n",
        "* `API_KEY`\n",
        "* `API_SECRET`\n",
        "\n",
        "On our object `auth` we are going to use the method called `set_access_token` that takes in the followng arguments.\n",
        "* `ACCESS_TOKEN`\n",
        "* `ACCESS_TOKEN_SECRET`\n",
        "\n",
        "We are then going to create an `api` object and pass in the `authentication`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Za99aMHa0vKg"
      },
      "outputs": [],
      "source": [
        "auth = tp.OAuthHandler(Keys.API_KEY, Keys.API_SECRET)\n",
        "auth.set_access_token(Keys.ACCESS_TOKEN, Keys.ACCESS_TOKEN_SECRET)\n",
        "api = tp.API(auth, wait_on_rate_limit=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4EiIM5A0vKh"
      },
      "source": [
        "### Querying data on Twitter\n",
        "\n",
        "We are going to scrap the data based on twitter which is related to service delivery. So our query(q) will be based on key word search that include  `servicedelivery` on our tweets it can be hashtags. We are going to mention the count and use the tweepy `Cursor` class to retrieve the `count` tweets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxH8F4P70vKh"
      },
      "outputs": [],
      "source": [
        "COUNT = 500_000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp_lK_Oa0vKi"
      },
      "source": [
        "### Data Class Tweet\n",
        "In the following code cell we are going to create a datatype called `Tweet`. This datatype will contain properties that we are interested in on a single `tweet` object. On each and every tweet we are going to be intrested with the following attributes:\n",
        "\n",
        "1. `id` - the tweet id\n",
        "\n",
        "2. `created_at` - the date a tweet was created\n",
        "\n",
        "3. `username` - who tweeted this tweet\n",
        "\n",
        "4. `text` - the text content of the tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3P0EhVj0vKi"
      },
      "outputs": [],
      "source": [
        "class Tweet:\n",
        "    def __init__(self, id:str, created_at:str, username:str, text:str):\n",
        "        self.id = id\n",
        "        self.created_at = created_at\n",
        "        self.username = username\n",
        "        self.text = text\n",
        "    \n",
        "    def __str__(self):\n",
        "        return f\"Tweet <{self.id}>\"\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return f\"Tweet <{self.id}>\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = list()\n",
        "for tweet in tp.Cursor(api.search_tweets, q=\"Service Delivery\",lang=\"en\",tweet_mode=\"extended\", count=200).items(COUNT):\n",
        "  tweet = tweet._json\n",
        "  data = (tweet['id'], tweet['created_at'], tweet['user']['screen_name'], tweet['full_text'])\n",
        "  tweets.append(Tweet(*data))\n"
      ],
      "metadata": {
        "id": "I8HTmo34e-eZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sQ4N4fA0vKl"
      },
      "source": [
        "### Checking a single `tweet` example\n",
        "In the following code cell we are going to check a single tweet example."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets[0].text"
      ],
      "metadata": {
        "id": "bWw92CGIhYyQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "90280860-6a51-47cd-d702-4cd6914754dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'@delhivery i want to place one order https://t.co/Ku1BvIFQWt Delivery courier wismaster shipment delivered to me but status cancelled mark updated 2 day please help me delhivery courier service  AWB NO 5963192046766 Delhivery courier DC HEAD IS MISVIHEBAR TALKING TO ME and no https://t.co/vWCdBOPK7A'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tweets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BJf0TU_iTpW",
        "outputId": "8619666a-d8a9-44ac-f753-4bbc56d29056"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17975"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIQX0VJ20vKn"
      },
      "source": [
        "\n",
        "\n",
        "### Features (text) Cleaning\n",
        "\n",
        "Our tweets was obtained using `scrapping` if we look at our example texts we can see that we have hastags `#`, mentions `@user`, numbers `123`, url's `http://google.com/whatever` etc. These things does not add any meaning to our text. We are going to create a preprocessing function that will be able to remove all hashtags, mentions, urls, numbers as well as expanding the words like `I'm` to `I am` so that we make our text so clean. We are also going to remove single letter that means nothing and convert everything word to lowercase for example let's have a look at the following sentence :\n",
        "\n",
        "```\n",
        "I'm working with a model downloaded on http://google.com/whatever #100daysofcode created using AI by @username5 e h in 2015.\n",
        "```\n",
        "\n",
        "When we clean this text we want it to look as follows:\n",
        "\n",
        "```\n",
        "i am working with a model downloaded on created using ai by in.\n",
        "```\n",
        "\n",
        "The above sentence does not make any sense to human being to to deep learning model it does make sense. Because deep learning models learns the context in the sentence not the meaning of the sentence.\n",
        "\n",
        "### Retweets\n",
        "\n",
        "We are going to remove retweets on our tweets. The retweets according to the tweetpy documentation are the tweets that start with `RT`. So we are going to remove them before we create our `.csv` file.\n",
        "\n",
        "### Is text cleaning going to improve model metrics?\n",
        "\n",
        "Cleaning features also known as a step to \"feature extration\" is a very important step in machine learning models. It helps us to reduce noise in our features so that our model instead of forcusing on learning numbers, hashtags and mentions it will just focus on the text which is what maters. This also reduces the size of the `vocabulary`. In NLP we have an important consept called `vocabulary` which i'm going to explain it more about it later in the model training notebook. But we must know that text cleaning reduces the size of the vocabulary and improve the model training speed.\n",
        "\n",
        "In the code cells that follows we are going to make use of the `nltk` and `re` packages to clean our text. We are going to create a function that will does the text cleaning for us and this function will be called `clean_sentence`. This function instead of removing just the noise from the sentence it will also expand contracted words such as `ain't` to `are not` for example.\n",
        "\n",
        "The text-cleaning functions was found on [CrispenGari/ml-utils](https://github.com/CrispenGari/ml-utils/tree/main/text-cleaning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOeLTkck0vKn"
      },
      "outputs": [],
      "source": [
        "def decontracted(phrase:str)->str:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        phrase (str): takes in a word like I'm\n",
        "\n",
        "    Returns:\n",
        "        string: a decontracted word like I am\n",
        "    \"\"\"\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkSohWIV0vKo"
      },
      "outputs": [],
      "source": [
        "def clean_sentence(sent:str)->str:\n",
        "\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        sent (str): an uncleaned sentence with text, punctuations, numbers and non-english words\n",
        "    Returns:\n",
        "        str: returns a cleaned sentence with only english words in it.\n",
        "    \"\"\"\n",
        "    sent = sent.lower() # converting the text to lower case\n",
        "    sent = re.sub(r'(@|#)([A-Za-z0-9]+)', ' ', sent) # removing tags and mentions (there's no right way of doing it with regular expression but this will try)\n",
        "    sent = re.sub(r\"([A-Za-z0-9]+[.-_])*[A-Za-z0-9]+@[A-Za-z0-9-]+(\\.[A-Z|a-z]{2,})+\", \" \", sent) # removing emails\n",
        "    sent = re.sub(r'https?\\S+', ' ', sent, flags=re.MULTILINE) # removing url's\n",
        "    sent = re.sub(r'\\d', ' ', sent) # removing none word characters\n",
        "    sent = re.sub(r'[^\\w\\s\\']', ' ', sent) # removing punctuations except for \"'\" in words like I'm\n",
        "    sent = re.sub(r'\\s+', ' ', sent).strip() # remove more than one space\n",
        "    words = list()\n",
        "    eng = set(nltk.corpus.words.words())\n",
        "    for word in sent.split(' '):\n",
        "        words.append(decontracted(word)) # replace word's like \"i'm -> i am\"\n",
        "    return \" \".join(w for w in words if w.lower() in eng or not w.isalpha()) # removing non-english words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_RVllQf0vKo"
      },
      "source": [
        "> TextBlob’s output for a polarity task is a float within the range `[-1.0, 1.0]` where `-1.0` is a negative polarity and `1.0` is positive. This score can also be equal to `0`, which stands for a neutral evaluation of a statement as it doesn’t contain any words from the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wY76CLeg0vKp"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "later on we will customize for neutral based on the absolute threshold to balance the data.\n",
        "\"\"\"\n",
        "\n",
        "def create_label(text:str)->str:\n",
        "    blob = TextBlob(text)\n",
        "    polarity =  blob.sentiment.polarity\n",
        "    if polarity == 0.0:\n",
        "        return \"neutral\"\n",
        "    elif polarity < 0.0:\n",
        "        return \"negative\"\n",
        "    else:\n",
        "        return \"positive\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Coe3PcLX0vKq",
        "outputId": "fc0298fb-2e5d-4693-c1d2-2a02a19913be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('positive', 'neutral', 'negative')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "create_label(\"This is lovely\"), create_label(\"I'm not saying anything\"), create_label(\"this is boring\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0QlrLGc0vKs"
      },
      "source": [
        "### Categorical Label\n",
        "\n",
        "Next we are going to create a categorical label based on the condition that if a given sentiment label is `negative` our categorical label will be `1` if our sentiment class label is `neutral` our categorical label will be `0` and `2` when it is positive. We are going to make create a python `lambda` function called `categorical_label`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2eLL5Rb0vKs"
      },
      "outputs": [],
      "source": [
        "categorical_label = lambda x: 1 if x == \"negative\" else 0 if x == \"neutral\" else 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Iyl-MWi0vKt"
      },
      "source": [
        "### Clean Tweets\n",
        "\n",
        "We are then going to clean our tweets text, give them categorical and class labels then save the the data in a `csv` file. Our `csv` file will have the following columns.\n",
        "\n",
        "1. `created_at` - the date when the tweet was created\n",
        "\n",
        "2. `text` - the tweet cleaned text\n",
        "\n",
        "3. `label` - the class label which can be `positive`, `negative` or `neutral`\n",
        "\n",
        "4. `categorical_label` - an integer value either `2`, `1` or `0` for `positive`, `negative` or `neutral` class labels respectively\n",
        "\n",
        "5. `username` - the user who wrote the tweet.\n",
        "\n",
        "6. `id` - the tweet id.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSPaT-a80vKt"
      },
      "outputs": [],
      "source": [
        "cleaned_tweets = list()\n",
        "\n",
        "for twt in tweets:\n",
        "  # Removing all retweets:\n",
        "  if twt.text.startswith(\"RT\"):\n",
        "    continue\n",
        "  else:\n",
        "    twt_txt = clean_sentence(twt.text)\n",
        "    twt_id = twt.id\n",
        "    twt_create_at = twt.created_at\n",
        "    twt_username = twt.username\n",
        "    \n",
        "    # labels\n",
        "    twt_label = create_label(twt_txt)\n",
        "    twt_categorical_label = categorical_label(twt_label)\n",
        "    \n",
        "    cleaned_tweets.append(tuple([twt_id, twt_create_at, twt_username, twt_txt, twt_label, twt_categorical_label]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(cleaned_tweets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nf5XyAmmXap",
        "outputId": "5f7cb1ff-ab0c-48ac-da93-5ed2f87b33e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6336"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm_6ecA60vKu"
      },
      "source": [
        "### Columns\n",
        "In the following code cell we are going to define the columns name that we are going to need in our `.csv` file as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-b9gDI2b0vKu"
      },
      "outputs": [],
      "source": [
        "columns = np.array(['id', 'created_at', 'username', 'text', 'label', 'categorical_label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Epi4Aor00vKu"
      },
      "source": [
        "### Dataframe\n",
        "In the following column we are going to create a dataframe base on our `cleaned_tweets` list and check the first `10` rows of our data as follows:\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAKBG0D10vKu",
        "outputId": "62724445-b68b-44d5-db4b-48a490691ba4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    id                      created_at         username  \\\n",
              "0  1547579068781998083  Thu Jul 14 13:49:37 +0000 2022    vivek49276338   \n",
              "1  1547579038197108737  Thu Jul 14 13:49:29 +0000 2022  henryki29684325   \n",
              "2  1547578977325158401  Thu Jul 14 13:49:15 +0000 2022      earifin_com   \n",
              "3  1547578768448884740  Thu Jul 14 13:48:25 +0000 2022       deAndrento   \n",
              "4  1547578665822593029  Thu Jul 14 13:48:01 +0000 2022     MatthewJRoth   \n",
              "5  1547578506283913217  Thu Jul 14 13:47:23 +0000 2022       ERadiators   \n",
              "6  1547578433789915137  Thu Jul 14 13:47:05 +0000 2022  flipkartsupport   \n",
              "7  1547578268529729537  Thu Jul 14 13:46:26 +0000 2022          col_fox   \n",
              "8  1547578177500852225  Thu Jul 14 13:46:04 +0000 2022   SparrowCareers   \n",
              "9  1547578161436577792  Thu Jul 14 13:46:00 +0000 2022      RoadsAgency   \n",
              "\n",
              "                                                text     label  \\\n",
              "0  i want to place one order delivery courier shi...   neutral   \n",
              "1  your delivery service is ducking and unreliabl...  negative   \n",
              "2  exclusive gift pack make your one feel differe...  positive   \n",
              "3  actually we should expect in the future alread...  positive   \n",
              "4  _z you only pay the at the delivery of your ca...  positive   \n",
              "5  read our latest review great customer service ...  positive   \n",
              "6  sorry about that we understand your concern ab...  negative   \n",
              "7  out visiting the rope access and wash reach wi...  positive   \n",
              "8  are you looking for a career where you contrib...   neutral   \n",
              "9  to strengthen with role in the mining sector w...   neutral   \n",
              "\n",
              "   categorical_label  \n",
              "0                  0  \n",
              "1                  1  \n",
              "2                  2  \n",
              "3                  2  \n",
              "4                  2  \n",
              "5                  2  \n",
              "6                  1  \n",
              "7                  2  \n",
              "8                  0  \n",
              "9                  0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7921fd5a-5648-4ed9-bf3e-8ca3d3dfaec4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>created_at</th>\n",
              "      <th>username</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>categorical_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1547579068781998083</td>\n",
              "      <td>Thu Jul 14 13:49:37 +0000 2022</td>\n",
              "      <td>vivek49276338</td>\n",
              "      <td>i want to place one order delivery courier shi...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1547579038197108737</td>\n",
              "      <td>Thu Jul 14 13:49:29 +0000 2022</td>\n",
              "      <td>henryki29684325</td>\n",
              "      <td>your delivery service is ducking and unreliabl...</td>\n",
              "      <td>negative</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1547578977325158401</td>\n",
              "      <td>Thu Jul 14 13:49:15 +0000 2022</td>\n",
              "      <td>earifin_com</td>\n",
              "      <td>exclusive gift pack make your one feel differe...</td>\n",
              "      <td>positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1547578768448884740</td>\n",
              "      <td>Thu Jul 14 13:48:25 +0000 2022</td>\n",
              "      <td>deAndrento</td>\n",
              "      <td>actually we should expect in the future alread...</td>\n",
              "      <td>positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1547578665822593029</td>\n",
              "      <td>Thu Jul 14 13:48:01 +0000 2022</td>\n",
              "      <td>MatthewJRoth</td>\n",
              "      <td>_z you only pay the at the delivery of your ca...</td>\n",
              "      <td>positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1547578506283913217</td>\n",
              "      <td>Thu Jul 14 13:47:23 +0000 2022</td>\n",
              "      <td>ERadiators</td>\n",
              "      <td>read our latest review great customer service ...</td>\n",
              "      <td>positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1547578433789915137</td>\n",
              "      <td>Thu Jul 14 13:47:05 +0000 2022</td>\n",
              "      <td>flipkartsupport</td>\n",
              "      <td>sorry about that we understand your concern ab...</td>\n",
              "      <td>negative</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1547578268529729537</td>\n",
              "      <td>Thu Jul 14 13:46:26 +0000 2022</td>\n",
              "      <td>col_fox</td>\n",
              "      <td>out visiting the rope access and wash reach wi...</td>\n",
              "      <td>positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1547578177500852225</td>\n",
              "      <td>Thu Jul 14 13:46:04 +0000 2022</td>\n",
              "      <td>SparrowCareers</td>\n",
              "      <td>are you looking for a career where you contrib...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1547578161436577792</td>\n",
              "      <td>Thu Jul 14 13:46:00 +0000 2022</td>\n",
              "      <td>RoadsAgency</td>\n",
              "      <td>to strengthen with role in the mining sector w...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7921fd5a-5648-4ed9-bf3e-8ca3d3dfaec4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7921fd5a-5648-4ed9-bf3e-8ca3d3dfaec4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7921fd5a-5648-4ed9-bf3e-8ca3d3dfaec4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "dataframe = pd.DataFrame(cleaned_tweets, columns=columns, index=None)\n",
        "dataframe.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpG2u_uQ0vKv"
      },
      "source": [
        "### Saving our data \n",
        "\n",
        "We are going to save our cleaned data in a `csv` file with the name `clean_tweets.csv` we are fong to use the pandas method on the dataframe called `to_csv` as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0u46901V0vKv",
        "outputId": "a7144363-06f0-4434-a745-6daf94a184e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ],
      "source": [
        "save_path = os.path.join(base_dir, 'clean_tweets.csv')\n",
        "\n",
        "dataframe.to_csv(save_path)\n",
        "\n",
        "print(\"Done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkyfQBHB0vKw"
      },
      "source": [
        "### Next\n",
        "\n",
        "In the next notebook we are going to create an `Artificial Neural Network (ANN)` model that will classifies our text into sentiment wether, positive, positive or nuetral. Given a text from the user our model will classify sentiments to positive, positive or nuetral."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDHCOqss0vKw"
      },
      "source": [
        "### References\n",
        "\n",
        "1. [tweepy docs](https://docs.tweepy.org/en/stable/authentication.html)\n",
        "2. [textblob-docs](https://textblob.readthedocs.io/en/dev/)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "00_Service_Delivery_Data_Scrapping.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}